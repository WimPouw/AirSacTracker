---
title: "Dynamic Circular Tracking in Animal Behaviors Using Hough Transfomations"
author: "Lara S. Burchardt & Wim Pouw"
date: "2022-12-14"
output:
  html_document:
    df_print: paged
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, collapse = TRUE, message = FALSE, echo = FALSE}

library(tidyverse)
library(effsize)
library(psych)
library(signal)
library(knitr)

butter.it <- function(x, samplingrate =  25, order = 4, lowpasscutoff = 10)
{bf <- butter(order,lowpasscutoff/(samplingrate/2), type="low") #normalized frequency by the nyquist limit
x <<- as.numeric(signal::filtfilt(bf, x))} 


curfol <- getwd() #set to source location by going to session -> set working directory
basefol <- dirname(curfol)


# one euro filter components #https://hal.inria.fr/hal-00670496/document #https://hal.inria.fr/hal-00670496/document
  smoothing_factor <- function(t_e, cutoff)
  {
    r <- 2*pi*cutoff*t_e
    r <- r / (r + 1)
    return(r)
  }
  
  exponential_smoothing <- function(a, x, x_prev)
  { 
    sm <- a * x + (1 - a) * x_prev
    return(sm)
  }
  
  OneEuroFilter <- function(t, x, min_cutoff, beta,
                   d_cutoff)
  {
    t_e = mean(c(diff(t), 0)) #the sampling period
    dx = c(diff(x), 0)
    # The filtered derivative of the signal.
    a_d = smoothing_factor(t_e, d_cutoff)
    dx = dx / t_e #change of the signal over time jumps, !!ASSUMES HERE A STABLE SAMPLING RATE!
    #get previous values that are the input for the exponential smoothing
    xprev <- c(x[-1], 0)
    dxprev <- c(dx[-1], 0)
    #
    dx_hat = exponential_smoothing(a_d, dx, dxprev)
    # The filtered signal.
    cutoff = min_cutoff +  beta * abs(dx_hat)
    a = smoothing_factor(t_e, cutoff)
    x_hat = exponential_smoothing(a, x, xprev)
    return(x_hat)
  }
  
   OneEurofiltfilt <- function(t, x, min_cutoff=2, beta=0.001,
                   d_cutoff=1)
   {
    smoothed  <- OneEuroFilter(t, x, min_cutoff, beta,
                   d_cutoff)
    smoothed <- OneEuroFilter(t, rev(smoothed), min_cutoff, beta,
                   d_cutoff)
    return(rev(smoothed))
   }
  
```


# Settings

We compare the manully tracked radii with the automatically tracked radii for 
different parameter settings. 
The following parameters are varied: 

alphas = [2, 2.5, 3]
betas = [25,30,35] 
dilations = [5,6,7] 
medianblurs = [25, 27, 29]  
threshs_div_1 = [8,10,12]
threshs_div_2 = [13,15,17]

The python codes can be found here:
https://github.com/WimPouw/AirSacTracker/blob/main/Script/AirSacTracking_cleaned_larawimv2.ipynb 

```{r processing_1, message = FALSE, echo = FALSE}

dataset_to_be_stored <- paste0(basefol, '/Manuscript/datasets_for_statistics/comparison_manual_tracked_v1.csv')

#merge mOuaz's data
modata <- list.files(paste0(basefol, '/TestFrames/manually_tracked_results/batches/'), pattern = '.csv')

mdat <- data.frame()
for(i in modata)
{
  mo <- read.csv(paste0(basefol, '/TestFrames/manually_tracked_results/batches/', i))
  mdat<- bind_rows(mdat, mo)
}
mdat$name <- sub("^.*:", "", mdat$Label)
mdat$name <- sub(".jpeg", "", mdat$name)

if(!file.exists(dataset_to_be_stored))
{
  man_data <- as.data.frame(mdat)
  #add transformation from perimeter to radius in manual data
  man_data$radius_man <- round(man_data$Perim./(2*pi))

  
  #only radii over 100 pixels are admitted (as below are non-tracked)
  man_data <- man_data %>% 
      dplyr::filter(radius_man > 100 | !is.na(radius_man))# now you only do this once, instead of over and over in the loop below (speed s-up things)
  
  # set path for comparison
  path <- paste0(basefol, "/TestResults/")
  pattern <- "*.csv"
  list_of_files <- list.files(path = path, pattern = pattern)
  
  # set up results dataframe
  
  df <- data.frame()
  
  for (a in 1:length(list_of_files)) 
  {
    
    auto_data <- read_delim(paste(path, list_of_files[a], sep = "\\"), delim = "," )
    
    #getting videoname
    videoname <- list_of_files[a]
    videoname <- substring(videoname, 1, nchar(videoname)-4)
    videoname <- str_split(videoname, pattern = "_")
    videoname <- videoname[[1]]
    videoname <- paste(videoname[14], videoname[15], sep = "_")
    
    auto_data$time <- auto_data$frame*(1/25)
    
    auto_data$smoothed_r <- butter.it(auto_data$r, samplingrate =  25, order = 4, lowpasscutoff = 10)
    auto_data$smoothed_r_euro <-  OneEurofiltfilt(t = auto_data$time, x = auto_data$r)
    
    # joining datasets
    joined_data <- left_join(auto_data, man_data, by = "name")
    # removing all lines, where no circle was tracked manually
    joined_data <- joined_data %>% dplyr::filter(r < 250)  #was not sure why
    joined_data <- joined_data %>% dplyr::filter(!is.na(radius_man))
    
    # run correlation
    
    cor_r       <- as.numeric(cor.test(joined_data$r, joined_data$radius_man)$estimate)
    cor_r_sm    <-  as.numeric(cor.test(joined_data$smoothed_r, joined_data$radius_man)$estimate)
    cor_r_sme   <-  as.numeric(cor.test(joined_data$smoothed_r_euro, joined_data$radius_man)$estimate)
    cor_x       <-  as.numeric(cor.test(joined_data$x, joined_data$X)$estimate)
    cor_y       <-  as.numeric(cor.test(joined_data$y, joined_data$Y)$estimate)
    
    #you can do by vector methods (no loop needed)
    diff <- round(joined_data$r - joined_data$radius_man, digits = 2)
    diff_sqr <- diff^2

  #get some statistics for these settings
  diff_min <- min(abs(diff))
  diff_max <- max(abs(diff))
  diff_avg <- mean(abs(diff))
  diff_median <- median(abs(diff))
  
  diff_sqr_min <- min(abs(diff_sqr))
  diff_sqr_max <- max(abs(diff_sqr))
  diff_sqr_avg <- mean(abs(diff_sqr))
  diff_sqr_median <- median(abs(diff_sqr))
  
  # save relevant parameters 
  # what do we need to save? 
  # video name, correlation, preprocessing parameters, how many frames in correlation from how many in the video
    
  #NOTE TO LARA: "in general, it is faster to bind vectors to a data frame at the end of a loop rather than adding values to a data frame within the loop. This is because modifying an existing data frame in R can be computationally expensive, as it requires creating a new object in memory to store the modified data frame. By contrast, appending new rows to a data frame using the bind_rows() function from the dplyr package is much more efficient, as it avoids the overhead of modifying the original data frame."
  
 df<- bind_rows(df, data.frame(videoname,cor_r, cor_r_sm,cor_x, cor_y, nrow(joined_data), nrow(auto_data),
   joined_data$alpha[1], joined_data$beta[1], joined_data$threh_div1[1], joined_data$threh_div2[1], joined_data$dilation[1],
   joined_data$medianblur[1], diff_min, diff_max, diff_avg, diff_median, diff_sqr_min, diff_sqr_max, diff_sqr_avg, diff_sqr_median,
  cor_r_sme))
  
  }
  colnames(df) <- c("videoname", "cor_radius","cor_radius_smoothed", "cor_x",  "cor_y", "nr_frames_used", "nr_frames_in_video",
                               "alpha","beta","thresh_div1","thresh_div2","dilation",
                               "medianblur", "diff_r_min", "diff_r_max","diff_r_avg","diff_r_median","diff_r_sqr_min",
                                "diff_r_sqr_max","diff_r_sqr_avg","diff_r_sqr_median", "cor_radius_smoothed_euro")

  write.csv(df, dataset_to_be_stored)
}

#if the file's version already exist than we dont need to recompute and can just read it in
#this will make the markdown quicker
if(file.exists(dataset_to_be_stored))
{
  df <- read.csv(dataset_to_be_stored)
}

```


# Best Videos and  effects of Smoothing

To check for which videos manual and automatic tracking matched best, 
we summarize correlation values per video. The results are shown in the table below.


```{r best_videos, echo=FALSE}
best_combos_low_10_video <- df %>% 
  group_by(videoname) %>%                            # multiple group columns
  summarise(mean_cor = mean(cor_radius_smoothed), median_cor = median(cor_radius_smoothed),
            min_cor = min(cor_radius_smoothed),max_cor = max(cor_radius), max_cor_butter = max(cor_radius_smoothed),
            max_cor_euro = max(cor_radius_smoothed_euro)) 


knitr::kable(best_combos_low_10_video)

```

# Best settings

Here we report on the effects of smoothing on the correlation between manually and automatically tracked circles.  

```{r smoothing, echo=FALSE}

best_combos_low_10 <- df %>% 
group_by(alpha, beta, thresh_div1, thresh_div2, dilation, medianblur) %>% summarise(mean_cor = mean(cor_radius_smoothed_euro), median_cor = median(cor_radius_smoothed_euro),
            min_cor = min(cor_radius_smoothed_euro), max_cor = max(cor_radius_smoothed_euro))

best_combos_low_10 <- best_combos_low_10[order(best_combos_low_10$mean_cor, decreasing =TRUE),]
best_combos_low_10 <-best_combos_low_10[1:10,] 
knitr::kable(best_combos_low_10)
#
#best_combos_low_euro <- df %>% 
#  group_by(alpha, beta, thresh_div1, thresh_div2, dilation, medianblur) %>% summarise(mean_cor = mean(cor_radius_smoothed_euro), #median_cor = median(cor_radius_smoothed),
#            min_cor = min(cor_radius_smoothed_euro), max_cor = max(cor_radius_smoothed_euro))
#
#knitr::kable(best_combos_low_euro)

```

# Best settings for each video




# Proof of Concept: Acoustic Analysis

We want to analyse some acoustic parameters of the boom call and song to the inflation of the airsacs.

To Dos:
- preparing Video and Audio files (Wim)
- extracting acoustic parameters
- analysing relation between acoustic parameters and airsack inflation



# Literature to check or include

1. Vocal size exaggeration may have contributed to the origins of vocalic complexity, https://doi.org/10.1098/rstb.2020.0401 
2.  Vocal tract length and formant frequency dispersion correlate with body size in rhesus macaques
W. T.  Fitch, DOI: 10.1121/1.421048
