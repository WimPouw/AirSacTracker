{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module to process video DLC model\n",
    "\n",
    "This module belongs to the manuscript \"Burchardt, L., Van de Sande, Y., Kehy, M., Gamba, M., Ravignani, A., Pouw, W. A complete computational and data toolkit for the dynamic study of laryngeal air sacs in Siamang (Symphalangus syndactylus) with applications for spherical tracking in other animals\".\n",
    "\n",
    "This contains a module for tracking Siamang head and air sack postures. The following keypoints will be tracked by a trained resnet 101 model:\n",
    "- UpperLip\n",
    "- LowerLip\n",
    "- Nose\n",
    "- EyeBridge\n",
    "- Start_outline_outer_left\n",
    "- Start_outline_outer_right\n",
    "- LowestPoint_outline\n",
    "- MidLowleft_outline\n",
    "- MidLowright_outline\n",
    "\n",
    "Note that Deeplabcut needs to be installed (in command prompt \"pip install -r requirements.txt\"). By default the CPU version is installed. Please see the original documentation of DeepLabCut to ensure GPU compatibility if you want to speed up the tracking process. We do recommend to use a GPU supported deeplabcut.\n",
    "\n",
    "The trained resnet101 model needs to be downloaded first from google drive; so please go to the following folder and follow the download link and download to that folder: \"./AirSacTracker/Toolkit/module_process_video_DLC_model/trained_model_and_metainfo/dlc-models/iteration-0/Deep_AirSacTrackingV1Jan1-trainset95shuffle1/train/\". \n",
    "\n",
    "## Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://tsg-131-174-75-200.hosting.ru.nl/samples_airsactoolkit/June16_02_circle_rec.mp4\" controls  width=\"600\"  height=\"500\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "video_url = 'https://tsg-131-174-75-200.hosting.ru.nl/samples_airsactoolkit/June16_02_circle_rec.mp4'  # Replace this with your video URL\n",
    "Video(video_url, width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.4...\n"
     ]
    }
   ],
   "source": [
    "# load in all the packages needed:\n",
    "\n",
    "# processing the videos with deeplabcut:\n",
    "import deeplabcut\n",
    "import os\n",
    "import shutil\n",
    "from os.path import isfile, join\n",
    "from IPython.display import Video\n",
    "\n",
    "# performing a circle estimation with Landau algorithm:\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# hough transformation\n",
    "import cv2  # image/video processing\n",
    "import pandas as pd  # data wranlging/csv\n",
    "from skimage import io, feature, color, measure, draw, img_as_float  # image processing\n",
    "import numpy as np  # data wrangling\n",
    "import os  # folder structuring\n",
    "from os.path import isfile, join  # for basic file operations\n",
    "from tqdm import tqdm  # for a process bar\n",
    "from IPython.display import Video  # for showing a video\n",
    "\n",
    "# plot landau circles on processed video:\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tabulate import tabulate\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: processing your video with DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained model settings\n",
    "config_path = \"./module_process_video_DLC_model/trained_model_and_metainfo/config.yaml\"\n",
    "\n",
    "# where are we going to save our tracked results to?\n",
    "output_dir = \"./module_process_video_DLC_model/results/replications_Y_07_23/\"\n",
    "\n",
    "# set videofolder from which we are going to process\n",
    "videofolder = \"./module_process_video_DLC_model/videos/\"\n",
    "\n",
    "# loading in the videos\n",
    "vids = [f for f in os.listdir(videofolder) if isfile(join(videofolder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./module_process_video_DLC_model/videos/example8.mp4\" controls  width=\"300\"  height=\"200\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first video in the set\n",
    "Video(videofolder + vids[0], width=300, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-500000 for model ./module_process_video_DLC_model/trained_model_and_metainfo/dlc-models/iteration-0/Deep_AirSacTrackingV1Jan1-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dlc/miniconda/envs/DEEPLABCUT_M1/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 10:25:35.682093: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  ./module_process_video_DLC_model/videos/example8.mp4\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "Found output file for scorer: DLC_resnet101_Deep_AirSacTrackingV1Jan1shuffle1_500000\n",
      "Converting ./module_process_video_DLC_model/results/replications_Y_07_23/example8DLC_resnet101_Deep_AirSacTrackingV1Jan1shuffle1_500000.h5...\n",
      "All H5 files were converted to CSV.\n"
     ]
    }
   ],
   "source": [
    "# loop through each video and track using DLC\n",
    "for i in vids:  # add the image folder name to get the full path\n",
    "    video_path = videofolder + i\n",
    "    # analyze the video using the pre-trained model\n",
    "    deeplabcut.analyze_videos(\n",
    "        config_path,\n",
    "        [video_path],\n",
    "        save_as_csv=False,\n",
    "        videotype=\".mp4\",\n",
    "        destfolder=output_dir,\n",
    "    )\n",
    "    # if you only want csv's than you uncomment the next line instead (note though that the labeling from deeplabcut requires .h5 instead of csv)\n",
    "    # deeplabcut.analyze_videos(config_path, [video_path],save_as_csv=False, videotype='.mp4', destfolder=output_dir)\n",
    "# convert H5 files to CSV files so you have the data in both extensions\n",
    "deeplabcut.analyze_videos_converth5_to_csv(output_dir, \".mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A Hough Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these are the files that you selected to be processed:\n",
      "['./module_circleestimation/data\\\\example8DLC_resnet101_Deep_AirSacTrackingV1Jan1shuffle1_500000.csv',\n",
      " './module_circleestimation/data\\\\_Opp_August_14_Session_1_zoom_syncedboom_2_2_FajarDLC_resnet101_Deep_AirSacTrackingV1Jan1shuffle1_500000.csv',\n",
      " './module_circleestimation/data\\\\_Opp_August_14_Session_1_zoom_syncedboom_5_3_FajarDLC_resnet101_Deep_AirSacTrackingV1Jan1shuffle1_500000.csv']\n"
     ]
    }
   ],
   "source": [
    "# load data:\n",
    "savename = \"example8\"\n",
    "path = \"./module_circleestimation/data/\"\n",
    "path_output = \"./module_circleestimation/results/replications_Y/\"\n",
    "timeseries_folder = \"./module_plotcirclesonvideos/timeseries/\"\n",
    "pattern = \"*.csv\"\n",
    "list_of_files = glob.glob(path + pattern)\n",
    "\n",
    "print(\"these are the files that you selected to be processed:\")\n",
    "print(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameter for which DLC points are taken into acount based on likelihood is higher than threshold\n",
    "threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateInitialGuessCircle(XY):\n",
    "    # estimate initial guess for circle LM\n",
    "    x0 = np.mean(XY[\"x\"].values)\n",
    "    y0 = np.mean(XY[\"y\"].values)\n",
    "    r0 = np.mean(\n",
    "        np.sqrt((XY[\"x\"].values ** 2 + x0**2) + (XY[\"y\"].values ** 2 + y0**2))\n",
    "    )\n",
    "    ParIni = [x0, y0, r0]\n",
    "    return ParIni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Landau(XY, ParIni=np.NAN, epsilon=0.0001, IterMax=800):\n",
    "    if np.isnan(ParIni):\n",
    "        ParIni = estimateInitialGuessCircle(XY)\n",
    "\n",
    "    centroidx = np.mean(XY[\"x\"].values)\n",
    "    centroidy = np.mean(XY[\"y\"].values)\n",
    "    centroid = [centroidx, centroidy]\n",
    "    X = XY[\"x\"].values - centroid[0]\n",
    "    Y = XY[\"y\"].values - centroid[1]\n",
    "    centroid = centroid + [0]\n",
    "\n",
    "    ParNew = [a - b for a, b in zip(ParIni, centroid)]\n",
    "\n",
    "    for i in range(0, IterMax + 1):\n",
    "        ParOld = ParNew\n",
    "        Dx = X - ParOld[0]\n",
    "        Dy = Y - ParOld[1]\n",
    "        Dx_squared = Dx * Dx\n",
    "        Dy_squared = Dy * Dy\n",
    "        D = np.sqrt([sum(x) for x in zip(Dx_squared, Dy_squared)])\n",
    "        ParNew = [\n",
    "            -np.mean(Dx / D) * np.mean(D),\n",
    "            -np.mean(Dy / D) * np.mean(D),\n",
    "            np.mean(D),\n",
    "        ]\n",
    "\n",
    "        progress = np.linalg.norm([new - old for new, old in zip(ParNew, ParOld)]) / (\n",
    "            np.linalg.norm(ParOld) + epsilon\n",
    "        )\n",
    "\n",
    "        if progress < epsilon:\n",
    "            break\n",
    "\n",
    "    Par = [sum(x) for x in zip(ParOld, centroid)]\n",
    "\n",
    "    return Par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_radius_estim_DLC(data):\n",
    "    def circle_format(df_sub):\n",
    "        df_all = []\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"x\", \"y\", \"likelihood\", \"frame\"])\n",
    "        for frame in range(1, df_sub.shape[0]):  # hier kan een .unique() flag achter\n",
    "            for b in range(\n",
    "                1, 6\n",
    "            ):  # R starts with 1 but is inclusive in endpoint, range in Python is exclusive on endpoint\n",
    "                end_col = b * 3\n",
    "                start_col = end_col - 3\n",
    "\n",
    "                helper_list = (\n",
    "                    df_sub.iloc[frame, start_col:end_col].values.flatten().tolist()\n",
    "                )\n",
    "                helper_list.append(frame)\n",
    "                df.loc[len(df)] = helper_list  # add data in row b in dataframe\n",
    "\n",
    "            df_all.append(df)\n",
    "\n",
    "        return pd.concat(df_all)\n",
    "\n",
    "    ## 01b: main ----\n",
    "\n",
    "    # list of columns needed for circle estimation used later in function\n",
    "    list_airsac_points = [\n",
    "        \"Start_outline_outer_left_x\",\n",
    "        \"Start_outline_outer_left_y\",\n",
    "        \"Start_outline_outer_left_likelihood\",\n",
    "        \"Start_outline_outer_right_x\",\n",
    "        \"Start_outline_outer_right_y\",\n",
    "        \"Start_outline_outer_right_likelihood\",\n",
    "        \"LowestPoint_outline_x\",\n",
    "        \"LowestPoint_outline_y\",\n",
    "        \"LowestPoint_outline_likelihood\",\n",
    "        \"MidLowleft_outline_x\",\n",
    "        \"MidLowleft_outline_y\",\n",
    "        \"MidLowleft_outline_likelihood\",\n",
    "        \"MidLowright_outline_x\",\n",
    "        \"MidLowright_outline_y\",\n",
    "        \"MidLowright_outline_likelihood\",\n",
    "    ]\n",
    "\n",
    "    colnames = []\n",
    "    colnames.append(\"frames\")  # first element is a string frames\n",
    "    for i in range(1, data.shape[1]):\n",
    "        colnames.append(\"_\".join([str(data.iloc[0, i]), str(data.iloc[1, i])]))\n",
    "\n",
    "    data.columns = colnames\n",
    "\n",
    "    df_all = data.iloc[2:, :]\n",
    "\n",
    "    df_sub = df_all.filter(list_airsac_points)\n",
    "    df_sub = df_sub.apply(pd.to_numeric)\n",
    "\n",
    "    circle_format_data = circle_format(df_sub)\n",
    "    return circle_format_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub functions for normalization\n",
    "\n",
    "\n",
    "def nose_eye_normalization(auto_data, min_frames=2, threshold_normalization=0.8):\n",
    "    norm_data = []\n",
    "    list_normalization_points = [\n",
    "        \"Nose_x\",\n",
    "        \"Nose_y\",\n",
    "        \"Nose_likelihood\",\n",
    "        \"EyeBridge_x\",\n",
    "        \"EyeBridge_y\",\n",
    "        \"EyeBridge_likelihood\",\n",
    "    ]\n",
    "\n",
    "    colnames = [\"frames\"]\n",
    "\n",
    "    for i in range(1, auto_data.shape[1]):\n",
    "        colnames.append(\n",
    "            \"_\".join([str(auto_data.iloc[0, i]), str(auto_data.iloc[1, i])])\n",
    "        )\n",
    "\n",
    "    auto_data.columns = colnames\n",
    "\n",
    "    df = auto_data.iloc[2:, :]\n",
    "    df_sub = df.filter(items=list_normalization_points)\n",
    "    df_sub = df_sub.apply(pd.to_numeric)\n",
    "\n",
    "    def euc_dist(xbridge, xnose, ybridge, ynose):\n",
    "        return np.sqrt((xbridge - xnose) ** 2 + (ybridge - ynose) ** 2)\n",
    "\n",
    "    df_sub_normalization = df_sub.loc[\n",
    "        df_sub[\"Nose_likelihood\"] >= threshold_normalization\n",
    "    ]\n",
    "    df_sub_normalization = df_sub_normalization.loc[\n",
    "        df_sub_normalization[\"EyeBridge_likelihood\"] >= threshold_normalization\n",
    "    ]\n",
    "\n",
    "    if df_sub_normalization.shape[0] >= min_frames:\n",
    "        distance = [\n",
    "            euc_dist(\n",
    "                df_sub_normalization[\"EyeBridge_x\"],\n",
    "                df_sub_normalization[\"Nose_x\"],\n",
    "                df_sub_normalization[\"EyeBridge_y\"],\n",
    "                df_sub_normalization[\"Nose_y\"],\n",
    "            )\n",
    "            for i in range(0, df_sub_normalization.shape[0])\n",
    "        ]\n",
    "    else:\n",
    "        distance = np.NAN\n",
    "\n",
    "    norm_data = np.nanmean(distance)\n",
    "\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_DLC_to_circle(path, list_of_files):\n",
    "    matrix = np.empty((0, 5))\n",
    "    columnnames = [\"radius\", \"x\", \"y\", \"frame\", \"videofile\"]\n",
    "    # radius_all = pd.DataFrame(matrix, columns= columnnames)\n",
    "    radius_all = []\n",
    "    normalization_value = pd.DataFrame(\n",
    "        data=np.empty((len(list_of_files), 2)),\n",
    "        columns=[\"normalization_value\", \"videofile\"],\n",
    "    )\n",
    "\n",
    "    for file in range(0, len(list_of_files)):\n",
    "        radius = pd.DataFrame(columns=columnnames)\n",
    "\n",
    "        auto_data = pd.read_csv(list_of_files[file])\n",
    "\n",
    "        data_circle_estimation = data_prep_radius_estim_DLC(data=auto_data)\n",
    "        data_circle_estimation = data_circle_estimation.astype({\"frame\": \"int\"})\n",
    "        grouped_data_circle_estimation = data_circle_estimation.loc[\n",
    "            data_circle_estimation[\"likelihood\"] > threshold\n",
    "        ]\n",
    "        grouped_data_circle_estimation = grouped_data_circle_estimation.groupby(\n",
    "            \"frame\", group_keys=False\n",
    "        )\n",
    "        count_n = 0\n",
    "        for name, group in grouped_data_circle_estimation:\n",
    "            if len(grouped_data_circle_estimation) > 0:\n",
    "                frame_data = group\n",
    "                if frame_data.shape[0] >= 3:\n",
    "                    circles_LAN = Landau(\n",
    "                        frame_data.iloc[:, 0:2],\n",
    "                        ParIni=np.NAN,\n",
    "                        epsilon=1e-06,\n",
    "                        IterMax=500,\n",
    "                    )\n",
    "                else:\n",
    "                    circles_LAN = [np.NAN, np.NAN, np.NAN, np.NAN]\n",
    "                circles_res = [\n",
    "                    circles_LAN[2],\n",
    "                    circles_LAN[0],\n",
    "                    circles_LAN[1],\n",
    "                    count_n,\n",
    "                    list_of_files[file],\n",
    "                ]\n",
    "\n",
    "                radius.loc[len(radius)] = circles_res\n",
    "\n",
    "            count_n += 1\n",
    "        radius_all.append(radius)\n",
    "        normalization_value[\"normalization_value\"][file] = nose_eye_normalization(\n",
    "            auto_data\n",
    "        )\n",
    "        normalization_value[\"videofile\"][file] = list_of_files[file]\n",
    "    radius_all = pd.concat(radius_all)\n",
    "    results_I = [radius_all, normalization_value]\n",
    "    results = pd.merge(results_I[0], results_I[1], how=\"left\", on=\"videofile\")\n",
    "\n",
    "    results[\"norm_radius\"] = pd.to_numeric(\n",
    "        results[\"radius\"] / results[\"normalization_value\"]\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29204\\2422804634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_DLC_to_circle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_of_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29204\\3864027043.py\u001b[0m in \u001b[0;36mfrom_DLC_to_circle\u001b[1;34m(path, list_of_files)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdata_circle_estimation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_circle_estimation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"int\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         grouped_data_circle_estimation = data_circle_estimation.loc[\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mdata_circle_estimation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"likelihood\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         ]\n\u001b[0;32m     21\u001b[0m         grouped_data_circle_estimation = grouped_data_circle_estimation.groupby(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "results = from_DLC_to_circle(path=path, list_of_files=list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(path_output + \"/\" + savename + \"_DLC_toRadii.csv\")\n",
    "results.to_csv(timeseries_folder + savename + \".csv\", na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan hier de hough transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module to process video with hough transform\n",
    "\n",
    "This module belongs to the manuscript \"Burchardt, L., Kehy, M., Gamba, M., Ravignani, A., Pouw, W. A complete computational and data toolkit for the dynamic study of laryngeal air sacs in Siamang (Symphalangus syndactylus) with applications for spherical tracking in other animals\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set videofolder\n",
    "videofolder = \"./module_hough/videos/\"\n",
    "outputfolder = \"./module_hough/results/replications_Y/\"\n",
    "# version 2, using videos #################### loading in the videos\n",
    "vids = [f for f in os.listdir(videofolder) if isfile(join(videofolder, f))]\n",
    "vidlist = []\n",
    "\n",
    "for i in vids:  # add the image folder name to get the full path\n",
    "    vidlist.append(videofolder + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./module_hough/videos/example8.mp4\" controls  width=\"300\"  height=\"200\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example video\n",
    "Video(videofolder + vids[0], width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preset settings preprocessing (thresh 1 and 2 are also weighted and then passed to hough transform)\n",
    "medianblur_preset = 27\n",
    "dilation_preset = 5\n",
    "alpha_preset = 2\n",
    "beta_preset = 30\n",
    "thresh_div_1_preset = 5\n",
    "thresh_div_2_preset = 14\n",
    "\n",
    "# hough presets\n",
    "dp_preset = 1\n",
    "minDist_preset = 10000\n",
    "maxRadius_preset = 270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(\n",
    "    image,\n",
    "    medianblur=medianblur_preset,\n",
    "    dilation=dilation_preset,\n",
    "    alpha=alpha_preset,\n",
    "    beta=beta_preset,\n",
    "    thresh_div_1=thresh_div_1_preset,\n",
    "    thresh_div_2=thresh_div_2_preset,\n",
    "):\n",
    "    # image0 = hougdraw(image)\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # brightness change\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    # set dynamic tresholds for canny (we will also pass this to the hough)\n",
    "    mean_intensity = np.median(gray)\n",
    "    threshold1 = int(max(0, (1.0 - 0.33) * mean_intensity / thresh_div_1))\n",
    "    threshold2 = int(min(255, (1.0 + 0.33) * mean_intensity / thresh_div_2))\n",
    "    # blur\n",
    "    image2 = cv2.medianBlur(gray, medianblur)\n",
    "    # dynamic thresholds for canny edge detection based on intensity of image\n",
    "    # Thresholds one standard deviation above and below median intensity\n",
    "    # edge detection\n",
    "    image3 = cv2.Canny(image2, threshold1, threshold2)\n",
    "    # dilation and second blur\n",
    "    submitted = cv2.dilate(image3, None, iterations=dilation)\n",
    "    image4 = cv2.medianBlur(submitted, medianblur)\n",
    "    # add hough\n",
    "    image4 = np.float32(image4)\n",
    "    return image4, threshold1, threshold2\n",
    "\n",
    "\n",
    "def preprocess_hough_apply_to_frame(image, mindist=10000, maxradius=250):\n",
    "    image, param1, param2 = preprocessing(image=image)\n",
    "    image = cv2.normalize(\n",
    "        src=image,\n",
    "        dst=None,\n",
    "        alpha=0,\n",
    "        beta=255,\n",
    "        norm_type=cv2.NORM_MINMAX,\n",
    "        dtype=cv2.CV_8U,\n",
    "    )\n",
    "    circles = cv2.HoughCircles(\n",
    "        image,\n",
    "        cv2.HOUGH_GRADIENT,\n",
    "        param1=param1,\n",
    "        param2=param2,\n",
    "        dp=dp_preset,\n",
    "        minDist=minDist_preset,\n",
    "        maxRadius=maxRadius_preset,\n",
    "    )\n",
    "    return circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop throuh video folder and process each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "Processing example8: 100%|██████████████████████████████████████████████████| 1195/1195 [01:43<00:00, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with processing video example8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "for video in vidlist:\n",
    "    name = os.path.basename(video)[0:-4]\n",
    "    # set up empty output dataframe\n",
    "    column_names = [\n",
    "        \"frame\",  # info on region of interest for repetability\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"r\",\n",
    "        \"namefr\",\n",
    "        \"sample_rate\",\n",
    "    ]  # parameters of hough circle transform\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    ####################set up video settings\n",
    "    cap = cv2.VideoCapture(video)  # set video to capture\n",
    "    frameWidth = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # frame width\n",
    "    frameHeight = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # frame height\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # fps = frames per second\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # number of frames\n",
    "    # set up video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(\n",
    "        *\"MP4V\"\n",
    "    )  # for different video formats you could use e.g., *'XVID'\n",
    "    out = cv2.VideoWriter(\n",
    "        outputfolder + name + \"_tracked.mp4\",\n",
    "        fourcc,\n",
    "        fps=fps,\n",
    "        frameSize=(int(frameWidth), int(frameHeight)),\n",
    "    )\n",
    "    ###################loop over frames of the original video\n",
    "    j = 0  # fame counter\n",
    "\n",
    "    # set up progress bar\n",
    "    with tqdm(\n",
    "        total=num_frames, desc=\"Processing \" + name, bar_format=\"{l_bar}{bar:50}{r_bar}\"\n",
    "    ) as pbar:\n",
    "        # the loop over frames (will close when no more frames are left to process)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "            j = j + 1  # add to the frame counter\n",
    "            namefr = \"framenr_\" + str(j) + \"_framevid_\" + os.path.basename(video[0:-4])\n",
    "            ############################detect circles\n",
    "            to_be_processed_frame = frame.copy()  # we keep the original frame\n",
    "            # apply hough\n",
    "            circles = preprocess_hough_apply_to_frame(to_be_processed_frame)\n",
    "            # draw the circles\n",
    "            if circles is not None:\n",
    "                circles = np.round(circles[0, 0:1]).astype(\"int\")\n",
    "                x = circles[0, 0]  # x  + plus the shift from the roi\n",
    "                y = circles[0, 1]  # y  + plus the shift from the roi\n",
    "                r = circles[0, 2]\n",
    "                cv2.circle(\n",
    "                    frame, (x, y), r, (255, 255, 0), 2\n",
    "                )  # version without drawing roi back on whole image\n",
    "                # save it to a row\n",
    "            if circles is None:\n",
    "                x = \"NA\"\n",
    "                y = \"NA\"\n",
    "                r = \"NA\"\n",
    "            # write frame\n",
    "            out.write(frame)  # save the frame to the new masked video\n",
    "            # write x,y,r data\n",
    "            new_row = [j, x, y, r, namefr, fps]\n",
    "            df.loc[len(df)] = new_row\n",
    "            # now update the progress bar\n",
    "            pbar.update(1)\n",
    "        # release video writer\n",
    "        out.release()\n",
    "        cap.release()\n",
    "        # save csv file with the timeseries results\n",
    "        df.to_csv(outputfolder + name + \".csv\", sep=\",\")\n",
    "    print(\"done with processing video \" + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dan als laatste de plotvideos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsfol = \"./module_plotcirclesonvideos/timeseries/\"  # this is where your timeseries are with the same name as the complementary video\n",
    "vidfol = \".module_plotcirclesonvideos/original_videos/\"  # this is where the original videos are, that need a circle added\n",
    "outfol = \".module_plotcirclesonvideos/output_videos_with_circles/\"  # this is where you can collect your output\n",
    "toprocess = os.listdir(tsfol)  # list all the time series files\n",
    "\n",
    "#Uncomment for absolute path Yana PC\n",
    "# tsfol = \"/Users/dlc/Documents/onderzoek/AirSacTracker/Toolkit/module_circleestimation/results/replications_Y/\"\n",
    "#vidfol = \"/Users/dlc/Documents/onderzoek/AirSacTracker/Toolkit/module_plotcirclesonvideos/original_videos/\"\n",
    "#outfol = \"/Users/dlc/Documents/onderzoek/AirSacTracker/Toolkit/module_plotcirclesonvideos/output_videos_with_circles/replications_Y_07_23/\"\n",
    "\n",
    "for tt in toprocess:\n",
    "    ts = pd.read_csv(tsfol + tt)  # get the time series\n",
    "    idname = tt[0 : len(tt) - 4]  # remove the .csv\n",
    "    vidloc = vidfol + idname + \".mp4\"  # add mp4 (we assume we only process mp4s!)\n",
    "    cap = cv2.VideoCapture(vidloc)  # open the video\n",
    "    frameWidth = cap.get(\n",
    "        cv2.CAP_PROP_FRAME_WIDTH\n",
    "    )  # get the framewidth, and use it for the new video\n",
    "    frameHeight = cap.get(\n",
    "        cv2.CAP_PROP_FRAME_HEIGHT\n",
    "    )  # get the framewidth, and use it for the new video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # fps = frames per second\n",
    "    # what should we write to?\n",
    "    out = cv2.VideoWriter(\n",
    "        outfol + idname + \"_circle.mp4\",\n",
    "        cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "        fps,\n",
    "        (int(frameWidth), int(frameHeight)),\n",
    "    )\n",
    "    print(\"working on video: \" + idname)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "        index_var = (\n",
    "            ts[\"frame\"] == frame_number\n",
    "        )  # get the index of the timeseries for the current frame number\n",
    "        dat = ts.loc[index_var, :]  # get the slice of data for this frame\n",
    "        for index, row in dat.iterrows():\n",
    "            if (\n",
    "                math.isnan(row[\"radius\"]) == False\n",
    "            ):  # only draw a circle when there a no NaN's\n",
    "                cv2.circle(\n",
    "                    frame,\n",
    "                    (int(row[\"x\"]), int(row[\"y\"])),\n",
    "                    int(row[\"radius\"]),\n",
    "                    (200, 0, 0),\n",
    "                    2,\n",
    "                )  # draw circle\n",
    "        out.write(frame)  # save it into a new frame\n",
    "\n",
    "# cleaning up\n",
    "out.release()  # release the output video\n",
    "cap.release()  # release the original video\n",
    "print(\n",
    "    \"We are all done, go look into your output folder: \" + str(os.path.abspath(outfol))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
