{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8982f1e",
   "metadata": {},
   "source": [
    "# Module to process video with hough transform\n",
    "\n",
    "This module belongs to the manuscript \"Burchardt, L., de Sande, Kehy, M., Gamba, M., Ravignani, A., Pouw, W. A complete computational and data toolkit for the dynamic study of laryngeal air sacs in Siamang (Symphalangus syndactylus) with applications for spherical tracking in other animals\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f17836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import cv2                       #image/video processing\n",
    "import pandas as pd              #data wranlging/csv\n",
    "from skimage import io, feature, color, measure, draw, img_as_float #image processing\n",
    "import numpy as np               #data wrangling\n",
    "import os                        #folder structuring\n",
    "from os.path import isfile, join #for basic file operations\n",
    "from tqdm import tqdm            #for a process bar\n",
    "from IPython.display import Video #for showing a video\n",
    "# set videofolder\n",
    "videofolder = './videos/'\n",
    "outputfolder = './results/'\n",
    "#version 2, using videos #################### loading in the videos\n",
    "vids = [f for f in os.listdir(videofolder) if isfile(join(videofolder, f))]\n",
    "vidlist = []\n",
    "\n",
    "for i in vids: #add the image folder name to get the full path\n",
    "    vidlist.append(videofolder +i) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7f0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./videos/green_tree_frog_example_test.mp4\" controls  width=\"300\"  height=\"200\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example video (Hyla cinerea)\n",
    "Video(videofolder+vids[0], width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405b78d",
   "metadata": {},
   "source": [
    "# Custom presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e618d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preset settings preprocessing (thresh 1 and 2 are also weighted and then passed to hough transform)\n",
    "medianblur_preset = 17\n",
    "dilation_preset = 8\n",
    "alpha_preset = 5\n",
    "beta_preset = 15\n",
    "thresh_div_1_preset = 26\n",
    "thresh_div_2_preset = 26\n",
    "\n",
    "# hough presets\n",
    "dp_preset = 1\n",
    "minDist_preset = 1000\n",
    "maxRadius_preset = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544025a3",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf57f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image, medianblur = medianblur_preset,\n",
    "                              dilation = dilation_preset,\n",
    "                              alpha = alpha_preset,\n",
    "                              beta= beta_preset,\n",
    "                              thresh_div_1= thresh_div_1_preset,\n",
    "                              thresh_div_2= thresh_div_2_preset):\n",
    "    #image0 = hougdraw(image)\n",
    "    #convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    #brightness change\n",
    "    gray = cv2.convertScaleAbs(gray, alpha = alpha, beta = beta)\n",
    "    #set dynamic tresholds for canny (we will also pass this to the hough)\n",
    "    mean_intensity = np.median(gray)\n",
    "    threshold1 = int(max(0, (1.0 - 0.33) * mean_intensity/thresh_div_1))\n",
    "    threshold2 = int(min(255, (1.0 + 0.33) * mean_intensity/thresh_div_2))    \n",
    "    #blur\n",
    "    image2 = cv2.medianBlur(gray, medianblur)\n",
    "    #dynamic thresholds for canny edge detection based on intensity of image\n",
    "    #Thresholds one standard deviation above and below median intensity\n",
    "    #edge detection\n",
    "    image3 = cv2.Canny(image2, threshold1, threshold2)\n",
    "    #dilation and second blur\n",
    "    submitted = cv2.dilate(image3, None, iterations= dilation)  \n",
    "    image4 = cv2.medianBlur(submitted, medianblur) \n",
    "    #add hough\n",
    "    image4 = np.float32(image4)\n",
    "    return image4, threshold1, threshold2\n",
    "\n",
    "def preprocess_hough_apply_to_frame(image, mindist=10000, maxradius=250):\n",
    "    image, param1, param2 = preprocessing(image=image)\n",
    "    image = cv2.normalize(src= image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U) \n",
    "    circles = cv2.HoughCircles(image, cv2.HOUGH_GRADIENT,param1 = param1, param2 = param2, dp = dp_preset, minDist = minDist_preset, maxRadius = maxRadius_preset)   \n",
    "    return(circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a3fb7",
   "metadata": {},
   "source": [
    "# Loop throuh video folder and process each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ab8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing green_tree_frog_example_test: 100%|██████████████████████████████████████████████████| 244/244 [00:05<00:00,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with processing video green_tree_frog_example_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "for video in vidlist:\n",
    "    name = os.path.basename(video)[0:-4]\n",
    "    #set up empty output dataframe\n",
    "    column_names = ['frame', # info on region of interest for repetability\n",
    "                            'x','y', 'r', 'namefr', 'sample_rate']# parameters of hough circle transform \n",
    "    df = pd.DataFrame(columns = column_names)  \n",
    "    ####################set up video settings\n",
    "    cap = cv2.VideoCapture(video)                       #set video to capture\n",
    "    frameWidth = cap.get(cv2.CAP_PROP_FRAME_WIDTH)      #frame width\n",
    "    frameHeight = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)    #frame height\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)                     #fps = frames per second\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #number of frames\n",
    "            #set up video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V') #for different video formats you could use e.g., *'XVID'\n",
    "    out = cv2.VideoWriter('./results/' + name + '_tracked.mp4', fourcc, \n",
    "                          fps = fps, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "    ###################loop over frames of the original video\n",
    "    j = 0   #fame counter\n",
    "    \n",
    "    #set up progress bar\n",
    "    with tqdm(total=num_frames, desc=\"Processing \" + name, bar_format=\"{l_bar}{bar:50}{r_bar}\") as pbar:\n",
    "        #the loop over frames (will close when no more frames are left to process)\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "            j=j+1 #add to the frame counter\n",
    "            namefr = 'framenr_' + str(j) + '_framevid_' + os.path.basename(video[0:-4])\n",
    "            ############################detect circles   \n",
    "            to_be_processed_frame=frame.copy() #we keep the original frame\n",
    "            #apply hough\n",
    "            circles = preprocess_hough_apply_to_frame(to_be_processed_frame)        \n",
    "            #draw the circles\n",
    "            if circles is not None:\n",
    "                circles = np.round(circles[0, 0:1]).astype(\"int\")\n",
    "                x = circles[0,0] #x  + plus the shift from the roi\n",
    "                y = circles[0,1] #y  + plus the shift from the roi\n",
    "                r = circles[0,2]\n",
    "                cv2.circle(frame, (x, y), r, (255, 255, 0), 2) #version without drawing roi back on whole image \n",
    "                #save it to a row\n",
    "            if circles is None:\n",
    "                x = \"NA\"\n",
    "                y = \"NA\"\n",
    "                r = \"NA\"\n",
    "            #write frame\n",
    "            out.write(frame) #save the frame to the new masked video\n",
    "            #write x,y,r data\n",
    "            new_row = [j, x, y, r, namefr, fps]\n",
    "            df.loc[len(df)] = new_row\n",
    "            #now update the progress bar\n",
    "            pbar.update(1)\n",
    "        #release video writer\n",
    "        out.release()\n",
    "        cap.release()\n",
    "        #save csv file with the timeseries results\n",
    "        df.to_csv('./results/'+name+'.csv', sep = ',') \n",
    "    print('done with processing video ' + name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98106cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062eeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ba766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f8f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f608d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
